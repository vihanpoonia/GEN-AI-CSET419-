{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPpgj/DKrUkRourro+ZigJA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vihanpoonia/GEN-AI-CSET419-/blob/main/'LAB2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JO0HZYwG9AyC",
        "outputId": "1f64ee50-3ff1-4049-a1ca-df9343442dff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Epoch 1/30 | D_loss: 1.418 | D_acc: 53.08% | G_loss: 0.679\n",
            "Epoch 2/30 | D_loss: 1.392 | D_acc: 52.42% | G_loss: 0.731\n",
            "Epoch 3/30 | D_loss: 1.391 | D_acc: 56.55% | G_loss: 0.749\n",
            "Epoch 4/30 | D_loss: 1.386 | D_acc: 57.39% | G_loss: 0.758\n",
            "Epoch 5/30 | D_loss: 1.383 | D_acc: 61.23% | G_loss: 0.774\n",
            "Epoch 6/30 | D_loss: 1.368 | D_acc: 64.47% | G_loss: 0.793\n",
            "Epoch 7/30 | D_loss: 1.371 | D_acc: 65.29% | G_loss: 0.800\n",
            "Epoch 8/30 | D_loss: 1.359 | D_acc: 69.81% | G_loss: 0.822\n",
            "Epoch 9/30 | D_loss: 1.341 | D_acc: 70.20% | G_loss: 0.837\n",
            "Epoch 10/30 | D_loss: 1.351 | D_acc: 69.19% | G_loss: 0.839\n",
            "Epoch 11/30 | D_loss: 1.340 | D_acc: 69.93% | G_loss: 0.857\n",
            "Epoch 12/30 | D_loss: 1.312 | D_acc: 73.27% | G_loss: 0.898\n",
            "Epoch 13/30 | D_loss: 1.274 | D_acc: 74.71% | G_loss: 0.939\n",
            "Epoch 14/30 | D_loss: 1.241 | D_acc: 76.49% | G_loss: 1.002\n",
            "Epoch 15/30 | D_loss: 1.177 | D_acc: 79.92% | G_loss: 1.097\n",
            "Epoch 16/30 | D_loss: 1.094 | D_acc: 82.19% | G_loss: 1.290\n",
            "Epoch 17/30 | D_loss: 1.029 | D_acc: 84.70% | G_loss: 1.398\n",
            "Epoch 18/30 | D_loss: 0.723 | D_acc: 91.93% | G_loss: 2.000\n",
            "Epoch 19/30 | D_loss: 0.460 | D_acc: 97.81% | G_loss: 3.697\n",
            "Epoch 20/30 | D_loss: 0.658 | D_acc: 93.95% | G_loss: 2.773\n",
            "Epoch 21/30 | D_loss: 0.587 | D_acc: 95.03% | G_loss: 2.810\n",
            "Epoch 22/30 | D_loss: 0.411 | D_acc: 98.81% | G_loss: 4.088\n",
            "Epoch 23/30 | D_loss: 0.339 | D_acc: 99.99% | G_loss: 5.381\n",
            "Epoch 24/30 | D_loss: 0.408 | D_acc: 98.80% | G_loss: 4.586\n",
            "Epoch 25/30 | D_loss: 0.404 | D_acc: 98.85% | G_loss: 4.428\n",
            "Epoch 26/30 | D_loss: 0.362 | D_acc: 99.68% | G_loss: 4.940\n",
            "Epoch 27/30 | D_loss: 0.390 | D_acc: 99.13% | G_loss: 4.587\n",
            "Epoch 28/30 | D_loss: 0.334 | D_acc: 100.00% | G_loss: 6.626\n",
            "Epoch 29/30 | D_loss: 0.330 | D_acc: 100.00% | G_loss: 7.678\n",
            "Epoch 30/30 | D_loss: 0.347 | D_acc: 99.65% | G_loss: 8.052\n",
            "\n",
            "Label Distribution of Generated Images:\n",
            "Label 5: 100\n"
          ]
        }
      ],
      "source": [
        "# ======================================================\n",
        "# 1. IMPORT LIBRARIES\n",
        "# ======================================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, utils\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# ======================================================\n",
        "# 2. USER INPUT PARAMETERS\n",
        "# ======================================================\n",
        "dataset_choice = 'mnist'        # 'mnist' or 'fashion'\n",
        "epochs = 30\n",
        "batch_size = 128\n",
        "noise_dim = 100\n",
        "lr_G = 0.0002\n",
        "lr_D = 0.0001\n",
        "save_interval = 5\n",
        "\n",
        "# ======================================================\n",
        "# 3. DATASET LOADING\n",
        "# ======================================================\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "if dataset_choice == 'mnist':\n",
        "    dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "elif dataset_choice == 'fashion':\n",
        "    dataset = datasets.FashionMNIST('./data', train=True, download=True, transform=transform)\n",
        "else:\n",
        "    raise ValueError(\"Invalid dataset choice\")\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "img_shape = (1, 28, 28)\n",
        "\n",
        "# ======================================================\n",
        "# 4. OUTPUT FOLDERS\n",
        "# ======================================================\n",
        "os.makedirs(\"generated_samples\", exist_ok=True)\n",
        "os.makedirs(\"final_generated_images\", exist_ok=True)\n",
        "\n",
        "# ======================================================\n",
        "# 5. GENERATOR\n",
        "# ======================================================\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(noise_dim, 256),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(1024, int(np.prod(img_shape))),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        img = self.model(z)\n",
        "        return img.view(img.size(0), *img_shape)\n",
        "\n",
        "# ======================================================\n",
        "# 6. DISCRIMINATOR\n",
        "# ======================================================\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(int(np.prod(img_shape)), 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        img = img.view(img.size(0), -1)\n",
        "        return self.model(img)\n",
        "\n",
        "G = Generator().to(device)\n",
        "D = Discriminator().to(device)\n",
        "\n",
        "# ======================================================\n",
        "# 7. LOSS & OPTIMIZERS\n",
        "# ======================================================\n",
        "criterion = nn.BCELoss()\n",
        "optimizer_G = optim.Adam(G.parameters(), lr=lr_G, betas=(0.5, 0.999))\n",
        "optimizer_D = optim.Adam(D.parameters(), lr=lr_D, betas=(0.5, 0.999))\n",
        "\n",
        "# ======================================================\n",
        "# 8. TRAINING LOOP\n",
        "# ======================================================\n",
        "for epoch in range(1, epochs + 1):\n",
        "    D_loss_total, G_loss_total = 0.0, 0.0\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    for real_imgs, _ in dataloader:\n",
        "        real_imgs = real_imgs.to(device)\n",
        "        batch = real_imgs.size(0)\n",
        "\n",
        "        # Label smoothing\n",
        "        real_labels = torch.full((batch, 1), 0.9).to(device)\n",
        "        fake_labels = torch.zeros(batch, 1).to(device)\n",
        "\n",
        "        # --------------------\n",
        "        # Train Discriminator\n",
        "        # --------------------\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        real_loss = criterion(D(real_imgs), real_labels)\n",
        "\n",
        "        z = torch.randn(batch, noise_dim).to(device)\n",
        "        fake_imgs = G(z)\n",
        "        fake_loss = criterion(D(fake_imgs.detach()), fake_labels)\n",
        "\n",
        "        D_loss = real_loss + fake_loss\n",
        "        D_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # Accuracy\n",
        "        preds_real = (D(real_imgs) > 0.5).float()\n",
        "        preds_fake = (D(fake_imgs.detach()) < 0.5).float()\n",
        "        correct += preds_real.sum().item() + preds_fake.sum().item()\n",
        "        total += batch * 2\n",
        "\n",
        "        # --------------------\n",
        "        # Train Generator (TWICE, FIXED)\n",
        "        # --------------------\n",
        "        for _ in range(2):\n",
        "            optimizer_G.zero_grad()\n",
        "\n",
        "            z = torch.randn(batch, noise_dim).to(device)   # NEW noise\n",
        "            fake_imgs = G(z)                               # NEW graph\n",
        "\n",
        "            G_loss = criterion(D(fake_imgs), real_labels)\n",
        "            G_loss.backward()\n",
        "            optimizer_G.step()\n",
        "\n",
        "        D_loss_total += D_loss.item()\n",
        "        G_loss_total += G_loss.item()\n",
        "\n",
        "    D_acc = (correct / total) * 100\n",
        "\n",
        "    print(f\"Epoch {epoch}/{epochs} | \"\n",
        "          f\"D_loss: {D_loss_total/len(dataloader):.3f} | \"\n",
        "          f\"D_acc: {D_acc:.2f}% | \"\n",
        "          f\"G_loss: {G_loss_total/len(dataloader):.3f}\")\n",
        "\n",
        "    # Save generated samples\n",
        "    if epoch % save_interval == 0:\n",
        "        utils.save_image(fake_imgs[:25],\n",
        "                         f\"generated_samples/epoch_{epoch:02d}.png\",\n",
        "                         nrow=5,\n",
        "                         normalize=True)\n",
        "\n",
        "# ======================================================\n",
        "# 9. GENERATE FINAL 100 IMAGES\n",
        "# ======================================================\n",
        "z = torch.randn(100, noise_dim).to(device)\n",
        "final_images = G(z)\n",
        "\n",
        "for i in range(100):\n",
        "    utils.save_image(final_images[i],\n",
        "                     f\"final_generated_images/img_{i}.png\",\n",
        "                     normalize=True)\n",
        "\n",
        "# ======================================================\n",
        "# 10. SIMPLE CLASSIFIER\n",
        "# ======================================================\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(784, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "classifier = Classifier().to(device)\n",
        "optimizer_C = optim.Adam(classifier.parameters(), lr=0.001)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(3):\n",
        "    for imgs, labels in dataloader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer_C.zero_grad()\n",
        "        loss = loss_fn(classifier(imgs), labels)\n",
        "        loss.backward()\n",
        "        optimizer_C.step()\n",
        "\n",
        "# ======================================================\n",
        "# 11. LABEL PREDICTION\n",
        "# ======================================================\n",
        "with torch.no_grad():\n",
        "    preds = classifier(final_images).argmax(dim=1).cpu().numpy()\n",
        "\n",
        "label_counts = Counter(preds)\n",
        "\n",
        "print(\"\\nLabel Distribution of Generated Images:\")\n",
        "for label, count in sorted(label_counts.items()):\n",
        "    print(f\"Label {label}: {count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# 1. IMPORT LIBRARIES\n",
        "# ======================================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, utils\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# ======================================================\n",
        "# 2. USER INPUT PARAMETERS\n",
        "# ======================================================\n",
        "dataset_choice = 'amnist'   # aMNIST (Augmented MNIST)\n",
        "epochs = 30\n",
        "batch_size = 128\n",
        "noise_dim = 100\n",
        "lr_G = 0.0002\n",
        "lr_D = 0.0001\n",
        "save_interval = 5\n",
        "\n",
        "# ======================================================\n",
        "# 3. aMNIST DATASET LOADING (AUGMENTED MNIST)\n",
        "# ======================================================\n",
        "# aMNIST = MNIST + affine transformations\n",
        "transform_amnist = transforms.Compose([\n",
        "    transforms.RandomAffine(\n",
        "        degrees=30,            # random rotation\n",
        "        translate=(0.1, 0.1),  # random translation\n",
        "        scale=(0.8, 1.2)       # random scaling\n",
        "    ),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "dataset = datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform_amnist\n",
        ")\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "img_shape = (1, 28, 28)\n",
        "\n",
        "# ======================================================\n",
        "# 4. OUTPUT DIRECTORIES\n",
        "# ======================================================\n",
        "os.makedirs(\"generated_samples\", exist_ok=True)\n",
        "os.makedirs(\"final_generated_images\", exist_ok=True)\n",
        "\n",
        "# ======================================================\n",
        "# 5. GENERATOR NETWORK\n",
        "# ======================================================\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(noise_dim, 256),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(1024, int(np.prod(img_shape))),\n",
        "            nn.Tanh()  # Output range [-1, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        img = self.model(z)\n",
        "        return img.view(img.size(0), *img_shape)\n",
        "\n",
        "# ======================================================\n",
        "# 6. DISCRIMINATOR NETWORK\n",
        "# ======================================================\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(int(np.prod(img_shape)), 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        img = img.view(img.size(0), -1)\n",
        "        return self.model(img)\n",
        "\n",
        "G = Generator().to(device)\n",
        "D = Discriminator().to(device)\n",
        "\n",
        "# ======================================================\n",
        "# 7. LOSS FUNCTION & OPTIMIZERS\n",
        "# ======================================================\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "optimizer_G = optim.Adam(G.parameters(), lr=lr_G, betas=(0.5, 0.999))\n",
        "optimizer_D = optim.Adam(D.parameters(), lr=lr_D, betas=(0.5, 0.999))\n",
        "\n",
        "# ======================================================\n",
        "# 8. TRAINING LOOP\n",
        "# ======================================================\n",
        "for epoch in range(1, epochs + 1):\n",
        "    D_loss_total, G_loss_total = 0.0, 0.0\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    for real_imgs, _ in dataloader:\n",
        "        real_imgs = real_imgs.to(device)\n",
        "        batch = real_imgs.size(0)\n",
        "\n",
        "        # Label smoothing\n",
        "        real_labels = torch.full((batch, 1), 0.9).to(device)\n",
        "        fake_labels = torch.zeros(batch, 1).to(device)\n",
        "\n",
        "        # --------------------\n",
        "        # Train Discriminator\n",
        "        # --------------------\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        real_loss = criterion(D(real_imgs), real_labels)\n",
        "\n",
        "        z = torch.randn(batch, noise_dim).to(device)\n",
        "        fake_imgs = G(z)\n",
        "        fake_loss = criterion(D(fake_imgs.detach()), fake_labels)\n",
        "\n",
        "        D_loss = real_loss + fake_loss\n",
        "        D_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # Accuracy calculation\n",
        "        preds_real = (D(real_imgs) > 0.5).float()\n",
        "        preds_fake = (D(fake_imgs.detach()) < 0.5).float()\n",
        "        correct += preds_real.sum().item() + preds_fake.sum().item()\n",
        "        total += batch * 2\n",
        "\n",
        "        # --------------------\n",
        "        # Train Generator (twice, fixed)\n",
        "        # --------------------\n",
        "        for _ in range(2):\n",
        "            optimizer_G.zero_grad()\n",
        "\n",
        "            z = torch.randn(batch, noise_dim).to(device)\n",
        "            fake_imgs = G(z)\n",
        "\n",
        "            G_loss = criterion(D(fake_imgs), real_labels)\n",
        "            G_loss.backward()\n",
        "            optimizer_G.step()\n",
        "\n",
        "        D_loss_total += D_loss.item()\n",
        "        G_loss_total += G_loss.item()\n",
        "\n",
        "    D_acc = (correct / total) * 100\n",
        "\n",
        "    print(f\"Epoch {epoch}/{epochs} | \"\n",
        "          f\"D_loss: {D_loss_total/len(dataloader):.3f} | \"\n",
        "          f\"D_acc: {D_acc:.2f}% | \"\n",
        "          f\"G_loss: {G_loss_total/len(dataloader):.3f}\")\n",
        "\n",
        "    # Save generated samples\n",
        "    if epoch % save_interval == 0:\n",
        "        utils.save_image(\n",
        "            fake_imgs[:25],\n",
        "            f\"generated_samples/epoch_{epoch:02d}.png\",\n",
        "            nrow=5,\n",
        "            normalize=True\n",
        "        )\n",
        "\n",
        "# ======================================================\n",
        "# 9. GENERATE FINAL 100 IMAGES\n",
        "# ======================================================\n",
        "z = torch.randn(100, noise_dim).to(device)\n",
        "final_images = G(z)\n",
        "\n",
        "for i in range(100):\n",
        "    utils.save_image(\n",
        "        final_images[i],\n",
        "        f\"final_generated_images/img_{i}.png\",\n",
        "        normalize=True\n",
        "    )\n",
        "\n",
        "# ======================================================\n",
        "# 10. SIMPLE CLASSIFIER FOR LABEL PREDICTION\n",
        "# ======================================================\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(784, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "classifier = Classifier().to(device)\n",
        "optimizer_C = optim.Adam(classifier.parameters(), lr=0.001)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train classifier briefly\n",
        "for epoch in range(3):\n",
        "    for imgs, labels in dataloader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer_C.zero_grad()\n",
        "        loss = loss_fn(classifier(imgs), labels)\n",
        "        loss.backward()\n",
        "        optimizer_C.step()\n",
        "\n",
        "# ======================================================\n",
        "# 11. PREDICT LABELS OF GENERATED aMNIST IMAGES\n",
        "# ======================================================\n",
        "with torch.no_grad():\n",
        "    preds = classifier(final_images).argmax(dim=1).cpu().numpy()\n",
        "\n",
        "label_counts = Counter(preds)\n",
        "\n",
        "print(\"\\nLabel Distribution of Generated aMNIST Images:\")\n",
        "for label, count in sorted(label_counts.items()):\n",
        "    print(f\"Label {label}: {count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0ovWeLSDZ07",
        "outputId": "b6116205-7f46-46d6-9b10-901547e7321a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Epoch 1/30 | D_loss: 1.421 | D_acc: 50.74% | G_loss: 0.682\n",
            "Epoch 2/30 | D_loss: 1.390 | D_acc: 50.84% | G_loss: 0.737\n",
            "Epoch 3/30 | D_loss: 1.390 | D_acc: 53.86% | G_loss: 0.747\n",
            "Epoch 4/30 | D_loss: 1.386 | D_acc: 55.90% | G_loss: 0.761\n",
            "Epoch 5/30 | D_loss: 1.384 | D_acc: 59.87% | G_loss: 0.767\n",
            "Epoch 6/30 | D_loss: 1.370 | D_acc: 61.47% | G_loss: 0.787\n",
            "Epoch 7/30 | D_loss: 1.376 | D_acc: 63.63% | G_loss: 0.790\n",
            "Epoch 8/30 | D_loss: 1.358 | D_acc: 67.75% | G_loss: 0.810\n",
            "Epoch 9/30 | D_loss: 1.341 | D_acc: 69.04% | G_loss: 0.830\n",
            "Epoch 10/30 | D_loss: 1.332 | D_acc: 70.27% | G_loss: 0.854\n",
            "Epoch 11/30 | D_loss: 1.327 | D_acc: 71.73% | G_loss: 0.871\n",
            "Epoch 12/30 | D_loss: 1.283 | D_acc: 75.49% | G_loss: 0.931\n",
            "Epoch 13/30 | D_loss: 1.279 | D_acc: 76.02% | G_loss: 0.936\n",
            "Epoch 14/30 | D_loss: 1.270 | D_acc: 77.10% | G_loss: 0.972\n",
            "Epoch 15/30 | D_loss: 1.209 | D_acc: 79.05% | G_loss: 1.073\n",
            "Epoch 16/30 | D_loss: 1.146 | D_acc: 81.24% | G_loss: 1.155\n",
            "Epoch 17/30 | D_loss: 1.116 | D_acc: 82.38% | G_loss: 1.246\n",
            "Epoch 18/30 | D_loss: 0.970 | D_acc: 87.28% | G_loss: 1.555\n",
            "Epoch 19/30 | D_loss: 0.793 | D_acc: 91.00% | G_loss: 1.926\n",
            "Epoch 20/30 | D_loss: 0.848 | D_acc: 90.13% | G_loss: 1.846\n",
            "Epoch 21/30 | D_loss: 0.621 | D_acc: 94.80% | G_loss: 2.485\n",
            "Epoch 22/30 | D_loss: 0.450 | D_acc: 98.32% | G_loss: 3.430\n",
            "Epoch 23/30 | D_loss: 0.486 | D_acc: 97.44% | G_loss: 3.343\n",
            "Epoch 24/30 | D_loss: 0.433 | D_acc: 98.40% | G_loss: 3.557\n",
            "Epoch 25/30 | D_loss: 0.401 | D_acc: 99.03% | G_loss: 4.000\n",
            "Epoch 26/30 | D_loss: 0.347 | D_acc: 99.97% | G_loss: 5.056\n",
            "Epoch 27/30 | D_loss: 0.344 | D_acc: 99.94% | G_loss: 5.023\n",
            "Epoch 28/30 | D_loss: 0.336 | D_acc: 99.99% | G_loss: 5.804\n",
            "Epoch 29/30 | D_loss: 0.351 | D_acc: 99.82% | G_loss: 5.431\n",
            "Epoch 30/30 | D_loss: 0.368 | D_acc: 99.58% | G_loss: 5.546\n"
          ]
        }
      ]
    }
  ]
}